<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think </title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Representation Entanglement for Generation: Training Diffusion Transformers Is Much Easier Than You Think</h1>
            <div class="is-size-5 publication-authors">
              <!-- 更新后的作者列表 -->
              <span class="author-block">
                <a href="#" target="_blank">Ge Wu<sup>1</sup></a>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Shen Zhang<sup>3</sup></a>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Ruijing Shi<sup>1</sup></a>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Shanghua Gao<sup>4</sup></a>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Zhenyuan Chen<sup>1</sup></a>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Lei Wang<sup>1</sup></a>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Zhaowei Chen<sup>2</sup></a>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Hongcheng Gao<sup>5</sup></a>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Yao Tang<sup>3</sup></a>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Jian Yang<sup>1</sup></a>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Ming-Ming Cheng<sup>1,2</sup></a>,</span>
              <span class="author-block">
                <a href="#" target="_blank">Xiang Li<sup>1</sup></a>
              </span>
            </div>
  
            <div class="is-size-5 publication-authors">
              <!-- 更新后的机构信息 -->
              <span class="author-block">
                <sup>1</sup>VCIP, CS, Nankai University, 
                <sup>2</sup>NKIARI, Shenzhen Futian, 
                <sup>3</sup>JIIOV Technology, 
                <sup>4</sup>Harvard University, 
                <sup>5</sup>University of Chinese Academy of Sciences
              </span>
              <!-- 移除了等贡献标记 -->
            </div>
  
            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF链接 -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2507.01467" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
  
                <!-- Supplementary PDF链接 -->
                <span class="link-block">
                  <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Supplementary</span>
                  </a>
                </span>
  
                <!-- Github链接 -->
                <span class="link-block">
                  <a href="https://github.com/Martinser/REG" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
  
                <!-- ArXiv摘要链接 -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2507.01467" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


<!-- Teaser image -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- 替换为图片标签 -->
      <img 
        src="static/images/reg_final.jpg"  
        alt="Description of your image"      
      >
      <h2 class="subtitle has-text-centered"></h2>
      <div class="content has-text-justified">
        <p>
          Figure 1. Comparison between REPA and our Representation Entanglement for Generation (REG).
          (a) During training, REPA indirectly aligns the intermediate denoising features of SiT with pretrained foundation model representations during training.
          (b) The external alignment of REPA is absent during actual denoising inference, limiting the effectiveness of discriminative information.
          (c) REG entangles low-level image latents with a pretrained foundation model's class token in training, providing discriminative semantic guidance to SiT. 
          (d) REG's inference process jointly reconstructs both image latents and their associated global semantics from random noise initialization. The incorporated semantic knowledge continues to guide generation, actively enhancing image quality during inference.
          (e) On ImageNet 256&times;256, SiT-XL/2 + REG achieves substantial acceleration in convergence, training <b>63</b>&times; and <b>23</b>&times; faster than SiT-XL/2 and SiT-XL/2 + REPA, respectively.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End teaser image -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            REPA and its variants effectively mitigate training challenges in diffusion models by incorporating external visual representations from pretrained models, through alignment between the noisy hidden projections of denoising networks and foundational clean image representations. We argue that the external alignment, which is absent during the entire denoising inference process, falls short of fully harnessing the potential of discriminative representations. In this work, we propose a straightforward method called <i><b>R</b>epresentation <b>E</b>ntanglement for <b>G</b>eneration</i> (<b>REG</b>), which entangles low-level image latents with a single high-level class token from pretrained foundation models for denoising. 
            REG acquires the capability to produce coherent image-class pairs directly from pure noise, substantially improving both generation quality and training efficiency.
            This is accomplished with negligible additional inference overhead, requiring only one single additional token for denoising (&lt;0.5% increase in FLOPs and latency).
            The inference process concurrently reconstructs both image latents and their corresponding global semantics, where the acquired semantic knowledge actively guides and enhances the image generation process.
            On ImageNet 256&times;256, SiT-XL/2 + REG demonstrates remarkable convergence acceleration, achieving <b>63</b>&times; and <b>23</b>&times; faster training than SiT-XL/2 and SiT-XL/2 + REPA, respectively. 
            More impressively, SiT-L/2 + REG trained for merely 400K iterations outperforms SiT-XL/2 + REPA trained for 4M iterations (<b>10</b>&times; longer).
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper contributions -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Contributions</h2>
        <div class="content has-text-justified">
          <ul>
            <li>We propose <b>REG</b>, an efficient framework that entangles low-level image latents with a single high-level class token from pretrained foundation models for denoising.</li>
            <li>REG significantly enhances generation quality, training convergence speed, and discriminative semantic learning while introducing negligible computational overhead.</li>
            <li>On ImageNet generation benchmarks, REG achieves <b>63</b>&times; and <b>23</b>&times; faster training convergence than SiT and REPA.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper contributions -->


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">A Quick Overview of Experimental Results</h2>
        <div class="content has-text-left is-size-5">
        <strong>Accelerating training convergence</strong>
        </div>
        <figure>
        <img src="static/images/table1.png" alt="fail" width="60%"">
        <figcaption class="content has-text-left" style="word-break:normal">
          <i>Table 1. <b>FID comparison across training iterations for accelerated alignment methods.</b> All experiments are conducted on ImageNet 256&times;256 without classifier-free guidance (CFG).</i>
        </figcaption>
        </figure>
        <br>
        <div class="content has-text-left is-size-5">
          <strong>Comparison with SOTA methods</strong>
        </div>
        <figure>
          <img src="static/images/table2.png" alt="fail" width="80%"">
          <figcaption class="content has-text-left" style="word-break:normal">
            <i>Table 2. <b>Comparison of the performance of different methods on ImageNet 256&times;256 with CFG.</b>
              Performance metrics are annotated with <span class="arrow">&uarr;</span> (higher is better) 
              and <span class="arrow">&darr;</span> (lower is better).</i> 
          </figcaption>
          </figure>
        <br>
        <div class="content has-text-left is-size-5">
          <strong>Partial ablation studies</strong>
        </div>
        <figure>
          <img src="static/images/table3.png" alt="fail" width="100%"">
          <figcaption class="content has-text-left" style="word-break:normal">
            <i>Table 3. Domain generalization experiments of three baselines with and without our ATPrompt on 4 datasets. Our method achieves
            consistent average performance improvement over three baseline methods.</i>
          </figcaption>
        </figure>
        <br>
        <div class="content has-text-left is-size-5">
          <strong>Analysis of discriminative semantic</strong>
        </div>
        <figure>
          <img src="static/images/fig_cknna.jpg" alt="fail" width="100%"">
          <figcaption class="content has-text-left" style="word-break:normal">
            <i>Figure 2. (a) Correlation between CKNNA and FID across training steps (color-coded by progression). REG demonstrates superior discriminative semantic learning, achieving higher CKNNA scores alongside lower FID values compared to REPA.
              (b) Layer-wise CKNNA progression at 400K training steps (t=0.5). REG consistently enhances CKNNA across all network layers, indicating robust discriminative semantics learning.
              (c) Timestep-wise CKNNA variation at 400K training. REG improves semantic alignment uniformly throughout the training process, outperforming baselines at all timesteps.</i> 
          </figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/castle_01.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          The visualization results of the class label is "Castle" 
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/bubble_01.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          The visualization results of the class label is "Bubble" 
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/dog1_01.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          The visualization results of the class label is "Border collie" 
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/cabinet_01.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        The visualization results of the class label is "China cabinet" 
      </h2>
    </div>
    <div class="item">
      <!-- Your image here -->
      <img src="static/images/great_grey_owl_01.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        The visualization results of the class label is "Great grey owl" 
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <p>
        If you find our paper is helpful for your research, please consider citing our paper.
      </p>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
